{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'C:\\Users\\ankit\\Desktop\\data_Sentimental\\IndiaFightsCorona1 with polarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 185)\t0.11609500015100332\n",
      "  (0, 819)\t0.2848277235166489\n",
      "  (0, 230)\t0.3081429985005696\n",
      "  (0, 886)\t0.30733190540416105\n",
      "  (0, 801)\t0.3092403714682543\n",
      "  (0, 650)\t0.23219837599040433\n",
      "  (0, 468)\t0.30951762094901075\n",
      "  (0, 522)\t0.276947336155005\n",
      "  (0, 660)\t0.3135280487276789\n",
      "  (0, 348)\t0.31382415877992414\n",
      "  (0, 407)\t0.31382415877992414\n",
      "  (0, 340)\t0.3141216178575646\n",
      "  (1, 395)\t0.18637811027882395\n",
      "  (1, 168)\t0.30561074840505503\n",
      "  (1, 583)\t0.44627138483763173\n",
      "  (1, 766)\t0.42408462660992674\n",
      "  (1, 844)\t0.33397016090115944\n",
      "  (1, 292)\t0.47247185548151993\n",
      "  (1, 350)\t0.36496988798244084\n",
      "  (1, 185)\t0.15776757996497945\n",
      "  (2, 736)\t0.317893803394285\n",
      "  (2, 571)\t0.310730162749087\n",
      "  (2, 61)\t0.3240985971165514\n",
      "  (2, 948)\t0.26126162777827794\n",
      "  (2, 163)\t0.21079535139066877\n",
      "  :\t:\n",
      "  (9893, 800)\t0.22728643811848542\n",
      "  (9893, 75)\t0.27268267990060785\n",
      "  (9893, 830)\t0.1963667553815684\n",
      "  (9894, 740)\t0.4374101060531897\n",
      "  (9894, 35)\t0.3866047073387546\n",
      "  (9894, 276)\t0.43080605492551327\n",
      "  (9894, 26)\t0.4056988189925709\n",
      "  (9894, 853)\t0.317542324287018\n",
      "  (9894, 731)\t0.31394272153649183\n",
      "  (9894, 318)\t0.26295206444674607\n",
      "  (9894, 334)\t0.20121347464187195\n",
      "  (9895, 635)\t0.456238057288626\n",
      "  (9895, 166)\t0.4550029420740821\n",
      "  (9895, 171)\t0.45378791092609877\n",
      "  (9895, 667)\t0.449116216721813\n",
      "  (9895, 169)\t0.30651382390952314\n",
      "  (9895, 390)\t0.2420823641570028\n",
      "  (9895, 395)\t0.1569576958188433\n",
      "  (9896, 529)\t0.38290257790319177\n",
      "  (9896, 824)\t0.41577061439748203\n",
      "  (9896, 857)\t0.3847579533535481\n",
      "  (9896, 159)\t0.4229496640794575\n",
      "  (9896, 929)\t0.3963129266558086\n",
      "  (9896, 811)\t0.2778062891396282\n",
      "  (9896, 748)\t0.3454779890309516\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[126   5   7]\n",
      " [ 51 866  65]\n",
      " [ 21  17 822]]\n"
     ]
    }
   ],
   "source": [
    "# Creating the bag of words model\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv = CountVectorizer()\n",
    "#X = cv.fit_transform((dataset['tweet_lemmatized'].values).astype('U')).toarray()\n",
    "#2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_vectorizer\n",
    "tfidf_stem = tfidf_vectorizer.fit_transform(dataset['tweet_lemmatized'].values.astype('U'))\n",
    "tfidf_stem\n",
    "print(tfidf_stem)\n",
    "X = tfidf_stem.toarray()\n",
    "print(X)\n",
    "\n",
    "y = dataset['polarity'].values\n",
    "\n",
    "# Splitting in Train and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.61616161616162\n"
     ]
    }
   ],
   "source": [
    "total_tweets_in_testset = np.sum(cm)\n",
    "correct_predictions = cm[0,0] + cm[1,1] + cm[2,2]\n",
    "accuracy = (correct_predictions/total_tweets_in_testset)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
